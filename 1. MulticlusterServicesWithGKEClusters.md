
# Multicluster Ingress Demo


**Multicluster Ingress** sets up a Global Loadbalancer that sends incoming traffic to the nearest cluster. Multicluster Ingress depends on another concept called **Multicluster Service** that stands up a headless kubernetes service for your application on each cluster. This headless kubernetes service has your application pods as endpoints. Each of these headless kubernetes services are set up as Network Endpoint Groups (NEGs) for the Global Loadbalancer's backend service. The traffic reaching the GLB will be sent to the nearest NED hence reaching your application pod.

In this demo we will set up two GKE clusters in different regions `us-central1-a` and `europe-west-1c`. We will create a MultiCluster Ingress and a MultiCluster Service that frontends an application to test
* traffic flow to the nearest cluster
* traffic failover to the other cluster when the application fails on the nearest cluster

## Prerequisites

* gcloud CLI

## Create two GKE Clusters in different regions

* Create a new project, if required  Set corresponding project id

```
PROJECT_ID=<<yourgcpproject>>
gcloud config set project ${PROJECT_ID}
```

* Enable required services in the project

```
gcloud services enable gkehub.googleapis.com
gcloud services enable anthos.googleapis.com
gcloud services enable multiclusteringress.googleapis.com
```

* Create `gke-us` in `us-central1-a` zone with Workload Identity enabled and IP Alias enabled

```
gcloud container clusters create gke-us --zone us-central1-a \
  --release-channel stable --enable-ip-alias \
  --workload-pool=${PROJECT_ID}.svc.id.goog
```
* Create `gke-eu` in `europe-west1-c` zone with Workload Identity enabled and IP Alias enabled

```
gcloud container clusters create gke-eu --zone europe-west1-c \
  --release-channel stable --enable-ip-alias \
  --workload-pool=${PROJECT_ID}.svc.id.goog
```


* Verify

```
gcloud container clusters list
```
Output similar to

```
NAME    LOCATION        MASTER_VERSION    MASTER_IP       MACHINE_TYPE  NODE_VERSION      NUM_NODES  STATUS
gke-eu  europe-west1-c  1.17.17-gke.3700  35.190.219.219  e2-medium     1.17.17-gke.3700  3          RUNNING
gke-us  us-central1-a   1.17.17-gke.3700  34.122.161.167  e2-medium     1.17.17-gke.3700  3          RUNNING
```

## Register clusters with Anthos

* Register the two clusters with Hub using Workload Identity

```
gcloud beta container hub memberships register gke-us  --gke-cluster=us-central1-a/gke-us --enable-workload-identity

gcloud beta container hub memberships register gke-eu  --gke-cluster=europe-west1-c/gke-eu  --enable-workload-identity
```

* Verify

```
gcloud container hub memberships list
```

to see output similar to

```
NAME    EXTERNAL_ID
gke-us  29125ca9-f0ad-47a0-b452-6626fa6c1808
gke-eu  9af671bf-f2b7-4f08-ba6f-42a6f4067a6e
```

## Setup Config Cluster for Multi Cloud Ingress

* Let us choose `gke-us` as the config cluster

```
gcloud alpha container hub ingress enable \
  --config-membership=projects/${PROJECT_ID}/locations/global/memberships/gke-us

```
to see output similar to

```
Waiting for Feature Ingress to be created...done.                                                  
Waiting for controller to start......done. 
```

##

* Create common namespace on both the clusters

```
kubectx gke-us
kubectl apply -f application1/namespace.yaml 
kubectx gke-eu
kubectl apply -f application1/namespace.yaml 
```

* Deploy the application container

```
kubectx gke-us
kubectl apply -f application1/deployment.yaml 
kubectx gke-eu
kubectl apply -f application1/deployment.yaml 

```
* Verify the pods are running

```
kubectx gke-us
kubectl get po -n zoneprinter 
kubectx gke-eu
kubectl get po -n zoneprinter
```

to see output (on both clusters) similar to 
```
NAME                            READY   STATUS    RESTARTS   AGE
zone-ingress-55dbfcf7d8-bv92j   1/1     Running   0          45s
```

```
kubectl apply -f application1/namespace.yaml
```


## Create Multicluster Service

```
kubectx gke-us
kubectl apply -f application1/mcs.yaml
```

* Verify
```
$ kubectl get multiclusterservices -n zoneprinter
NAME       AGE
zone-mcs   108s
```

* Verify there is a headless service on both clusters

```
kubectx gke-us
kubectl get svc -n zoneprinter

kubectx gke-eu
kubectl get svc -n zoneprinter
```

```
$ kubectl get svc mci-zone-mcs-svc-lgq966x5mxwwvvum -o
 yaml -n zoneprinter
apiVersion: v1
kind: Service
metadata:
  annotations:
    cloud.google.com/neg: '{"exposed_ports":{"8080":{}}}'
    cloud.google.com/neg-status: '{"network_endpoint_groups":{"8080":"k8s1-7d7605d2-zoneprint-mci-zo
ne-mcs-svc-lgq966x5m-808-bfaced62"},"zones":["europe-west1-c"]}'
    networking.gke.io/multiclusterservice-parent: '{"Namespace":"zoneprinter","Name":"zone-mcs"}'
  creationTimestamp: "2021-05-07T00:51:15Z"
  name: mci-zone-mcs-svc-lgq966x5mxwwvvum
  namespace: zoneprinter
  resourceVersion: "80851"
  selfLink: /api/v1/namespaces/zoneprinter/services/mci-zone-mcs-svc-lgq966x5mxwwvvum
  uid: cddd8338-a33b-414c-8d97-9bca400d46f8
spec:
  clusterIP: None
  ports:
  - name: web
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: zoneprinter
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

```
$ kubectl get svc mci-zone-mcs-svc-lgq966x5mxwwvvum -n
 zoneprinter -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    cloud.google.com/neg: '{"exposed_ports":{"8080":{}}}'
    cloud.google.com/neg-status: '{"network_endpoint_groups":{"8080":"k8s1-6fcf0b12-zoneprint-mci-zo
ne-mcs-svc-lgq966x5m-808-43fdadfd"},"zones":["us-central1-a"]}'
    networking.gke.io/multiclusterservice-parent: '{"Namespace":"zoneprinter","Name":"zone-mcs"}'
  creationTimestamp: "2021-05-07T00:51:14Z"
  name: mci-zone-mcs-svc-lgq966x5mxwwvvum
  namespace: zoneprinter
  resourceVersion: "79194"
  selfLink: /api/v1/namespaces/zoneprinter/services/mci-zone-mcs-svc-lgq966x5mxwwvvum
  uid: 74a5f76e-f11d-430e-a46b-300e16b4387d
spec:
  clusterIP: None
  ports:
  - name: web
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: zoneprinter
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

**Note:** the NEG annotations are created for the headless services


```
NAME                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
mci-zone-mcs-svc-lgq966x5mxwwvvum   ClusterIP   None         <none>        8080/TCP   2m39s
```

# Create Multi Cluster Ingress

```
kubectx gke-us
kubectl apply -f application1/mci.yaml 
```
output 
```
multiclusteringress.networking.gke.io/zone-ingress created
```

* Verify

```
$ kubectl get multiclusteringress -n zoneprinter
NAME           AGE
zone-ingress   2m32s
```

```
kubectl describe mci zone-ingress -n zoneprinter
```

* Check the NEGs are added and the NEG names match the annotations on the services added before by MCS.

```
$ gcloud compute network-endpoint-groups list
NAME                                                             LOCATION        ENDPOINT_TYPE   SIZE
k8s1-6fcf0b12-zoneprint-mci-zone-mcs-svc-lgq966x5m-808-43fdadfd  us-central1-a   GCE_VM_IP_PORT  1
k8s1-7d7605d2-zoneprint-mci-zone-mcs-svc-lgq966x5m-808-bfaced62  europe-west1-c  GCE_VM_IP_PORT  1
```

* Check the backend service. Note this backend service is pointing to the two NEGS

```
$ gcloud compute backend-services list
NAME                                  BACKENDS                                                      
                                                                                                    
                                        PROTOCOL
mci-9g29ne-8080-zoneprinter-zone-mcs  europe-west1-c/networkEndpointGroups/k8s1-7d7605d2-zoneprint-mci-zone-mcs-svc-lgq966x5m-808-bfaced62,us-central1-a/networkEndpointGroups/k8s1-6fcf0b12-zoneprint-mci-zone-mcs-svc-lgq966x5m-808-43fdadfd  HTTP
```

* Check the Load Balancer that is added by MCI to GCP. Note the LB is pointing to the backend service

```
gcloud compute url-maps list
```

output

```
NAME                                 DEFAULT_SERVICE
mci-9g29ne-zoneprinter-zone-ingress  backendServices/mci-9g29ne-8080-zoneprinter-zone-mcs
```

## Test

* Get VIP for the loadbalancer

```
VIP=$(kubectl get mci zone-ingress -n zoneprinter -o jsonpath='{.status.VIP}')
echo $VIP
```

* Test 

```
curl ${VIP}/ping
```
to observe output similar to 

```
curl ${VIP}/ping
```

You'll notice that the output comes from the cluster in the region nearest to you. In my case it is coming from `us-central1-a`

```
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf
7d8-bv92j"}
```


### Test from a different region

* Spin up a VM in the other region (in my case in europe-west1)


```
gcloud beta compute --project=${PROJECT_ID} \
instances create test-vm --zone=europe-west1-c \
 --machine-type=e2-micro --image=debian-10-buster-v20210420 \
 --image-project=debian-cloud --boot-disk-size=10GB \
 --boot-disk-type=pd-balanced --boot-disk-device-name=test-vm
```

* SSH into this VM to test

```
gcloud compute ssh test-vm --zone=europe-west1-c
```

* Test from this VM

```
VIP=<<virtualip from zoneingress>
curl ${VIP}/ping
```

to see output from the pod running in `europe-west1-c`

```
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfc
f7d8-kxjm9"}
```

* Exit from the test-vm and delete it

```
exit
```

```
gcloud compute instances delete test-vm --zone=europe-west1-c
```

### Test failover

Let's test by killing the pod in our nearest region.

```
kubectx gke-us
kubectl get po -n zoneprinter
```

See the pod running

```
NAME                            READY   STATUS    RESTARTS   AGE
zone-ingress-55dbfcf7d8-bv92j   1/1     Running   0          68m
```

Let us open a different terminal window and run the curl in a loop. You'll have to set the value of `VIP` in that window.

```
while true; do curl ${VIP}/ping; echo; done
```

Keep watching the output in this window. All the traffic currently goes to the cluster in `us-central1-a` 

```
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
....

```

Now let's kill the pod in the `us-central1-a` region to see what happens. From the other terminal, let us run

```
PODNAME=$(kubectl get po -n zoneprinter -o jsonpath='{.items[].metadata.name}')
kubectl delete po $PODNAME -n zoneprinter
```

As soon as the pod is killed, look at the other window to see the traffic getting redirected to `europe-west1-c` region, for a little while until a replacement pod comes up in `us-central1-a`

```
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"us-central1-a","Backend":"zone-ingress-55dbfcf7d8-bv92j"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfcf7d8-kxjm9"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfcf7d8-kxjm9"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfcf7d8-kxjm9"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfcf7d8-kxjm9"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfcf7d8-kxjm9"}
{"Hostname":"35.190.18.95","Version":"1.0","GCPZone":"europe-west1-c","Backend":"zone-ingress-55dbfcf7d8-kxjm9"}
...

```


## Cleanup

* Delete MCI

```
kubectx gke-us
kubectl delete multiclusteringress $(kubectl get multiclusteringress -n zoneprinter -o jsonpath='{.items[].metadata.name}') -n zoneprinter
```

* Delete MCS

```
kubectl delete mcs $(kubectl get mcs -n zoneprinter -o jsonpath='{.items[].metadata.name}') -n zoneprinter
```

* Unregister Clusters

```
gcloud beta container hub memberships delete gke-eu
gcloud beta container hub memberships delete gke-us

```

* Delete Clusters
```
gcloud container clusters delete gke-eu --zone=europe-west1-c
gcloud container clusters delete gke-us --zone=us-central1-a
```